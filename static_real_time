import os
import numpy as np
import pandas as pd
import mediapipe as mp
import cv2
import json
import joblib
import tensorflow as tf
from tensorflow.keras.models import load_model

# Load scaler and model
scaler = joblib.load('scaler.pkl')
model = load_model("large_dataset_model.keras", compile=False)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# Initialize MediaPipe Hands to detect multiple hands
hands = mp_hands.Hands(
    static_image_mode=True, 
    min_detection_confidence=0.8, 
    max_num_hands=2  # Allow detection of up to 2 hands
)

# Total number of landmarks in MediaPipe Hands
NUM_LANDMARKS = 21  # MediaPipe Hands has 21 landmarks per hand
FEATURES_PER_LANDMARK = 2  # x and y coordinates

x_ = []
y_ = []

def normalize_landmarks(landmarks):
    """
    Normalize landmarks relative to the minimum x and y coordinates
    """
    x = [lm.x for lm in landmarks]
    y = [lm.y for lm in landmarks]
    min_x, min_y = min(x), min(y)

    for i in x:
        x_.append(i)
    for i in y:
        y_.append(i)
    
    normalized_landmarks = []
    for lm in landmarks:
        normalized_landmarks.extend([
            lm.x - min_x, 
            lm.y - min_y
        ])
    
    return normalized_landmarks

with open(r"label_map_numbers.json") as file:
    labels_dict = {int(k): v for k, v in json.load(file).items()}

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(frame_rgb)
    x_ = []
    y_ = []
    H, W, _ = frame.shape

    try:
        # Check if hand landmarks are detected
        if results.multi_hand_landmarks:
            # Sort hands by handedness to ensure consistent order
            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                mp_drawing.draw_landmarks(
                    frame,
                    hand_landmarks,
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing_styles.get_default_hand_landmarks_style(),
                    mp_drawing_styles.get_default_hand_connections_style()
                )
                
            # Get handedness information
            if results.multi_handedness:
                # Determine left and right hand
                hands_info = list(zip(results.multi_handedness, results.multi_hand_landmarks))
                
                # Sort hands by handedness to ensure consistent order (left first, then right)
                hands_info.sort(key=lambda x: x[0].classification[0].label)
                
                # Initialize landmarks for left and right hands
                left_hand_landmarks = None
                right_hand_landmarks = None
                
                # Assign hands to left and right
                for handedness, landmarks in hands_info:
                    if handedness.classification[0].label == 'Left':
                        left_hand_landmarks = landmarks
                    elif handedness.classification[0].label == 'Right':
                        right_hand_landmarks = landmarks
                
                # Prepare data for both hands
                normalized_landmarks = []
                
                # Normalize and add left hand landmarks (if exists)
                if left_hand_landmarks:
                    normalized_landmarks.extend(normalize_landmarks(left_hand_landmarks.landmark))
                else:
                    # If no left hand, add zeros
                    normalized_landmarks.extend([0] * (NUM_LANDMARKS * FEATURES_PER_LANDMARK))
                
                # Normalize and add right hand landmarks (if exists)
                if right_hand_landmarks:
                    normalized_landmarks.extend(normalize_landmarks(right_hand_landmarks.landmark))
                else:
                    # If no right hand, add zeros
                    normalized_landmarks.extend([0] * (NUM_LANDMARKS * FEATURES_PER_LANDMARK))
                
                # Ensure consistent number of features (84 total: 42 for left, 42 for right)
                if len(normalized_landmarks) != NUM_LANDMARKS * FEATURES_PER_LANDMARK * 2:
                    print("Can't process hands")
                    continue
    
            # Load the scaler (fitted during training)
            scaler = joblib.load('scaler.pkl')
    
            # Normalize and reshape the data to match the model's input format
            normalized_landmarks = np.array(normalized_landmarks).reshape(1, 84)  # Reshape to (1, 84) to match training data's flattened format
    
            # Apply scaling using the pre-trained scaler
            normalized_landmarks = scaler.transform(normalized_landmarks)
            normalized_landmarks = normalized_landmarks.reshape(normalized_landmarks.shape[0], normalized_landmarks.shape[1], 1)
    
            # Predict
            prediction = model.predict(normalized_landmarks)
            predicted_character = labels_dict[np.argmax(prediction)]
    
            # Draw rectangle and text
            x1 = int(min(x_) * W) - 10
            y1 = int(min(y_) * H) - 10
            x2 = int(max(x_) * W) + 10
            y2 = int(max(y_) * H) + 10
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)
            cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)
    
        cv2.imshow('Indian Sign Language Recognition', frame) 
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    except Exception as e:
        print(e)

cap.release()
hands.close()
cv2.destroyAllWindows()
